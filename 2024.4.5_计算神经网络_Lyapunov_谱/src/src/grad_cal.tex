\documentclass[12pt,a4paper]{amsart}
\usepackage[UTF8]{ctex}
\usepackage{preamble}


\title{计算两层神经网络参数迭代的 Jacobi 矩阵：以 mnist 数据集为例}

\begin{document}

\maketitle

\section{变量定义}

\subsection{输入输出}

\begin{enumerate}
	\item 学习率 $\alpha = 0.1$
	\item 一个 batch 的大小 $k = 60000$
	\item 输入层的输入 $X \in \R^{k \times 784}$

		\begin{equation}
			X = \begin{bmatrix}
				x^1    \\
				x^2    \\
				\vdots \\
				x^k
			\end{bmatrix} = \begin{bmatrix}
				x_1^1  & x_2^1  & \cdots & x_{784}^1 \\
				x_1^2  & x_2^2  & \cdots & x_{784}^2 \\
				\vdots & \vdots & \ddots & \vdots    \\
				x_1^k  & x_2^k  & \cdots & x_{784}^k
			\end{bmatrix}
		\end{equation}

	\item 隐藏层的输入 $A_1 \in \R^{k \times 50}$

		\begin{equation}
			A_1 = \begin{bmatrix}
				a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
				a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
				\vdots & \vdots & \ddots & \vdots    \\
				a_1^k  & a_2^k  & \cdots & a_{50}^k
			\end{bmatrix}
		\end{equation}

	\item 隐藏层的输出 $Z_1 \in \R^{k \times 50}$

		\begin{equation}
			Z_1 = \begin{bmatrix}
				z_1^1  & z_2^1  & \cdots & z_{50}^1 \\
				z_1^2  & z_2^2  & \cdots & z_{50}^2 \\
				\vdots & \vdots & \ddots & \vdots    \\
				z_1^k  & z_2^k  & \cdots & z_{50}^k
			\end{bmatrix}
		\end{equation}

	\item 输出层的输入 $A_2 \in \R^{k \times 10}$（$Z_1 = A_2$，是同一个矩阵）

		\begin{equation}
			A_2 = \begin{bmatrix}
				{a'}_1^1 & {a'}_2^1 & \cdots & {a'}_{10}^1 \\
				{a'}_1^2 & {a'}_2^2 & \cdots & {a'}_{10}^2 \\
				\vdots   & \vdots   & \ddots & \vdots      \\
				{a'}_1^k & {a'}_2^k & \cdots & {a'}_{10}^k
			\end{bmatrix}
		\end{equation}

	\item 输出层的输出 $Y \in \R^{k \times 10}$

		\begin{equation}
			Y = \begin{bmatrix}
				y^1    \\
				y^2    \\
				\vdots \\
				y^k
			\end{bmatrix} = \begin{bmatrix}
				y_1^1  & y_2^1  & \cdots & y_{10}^1 \\
				y_1^2  & y_2^2  & \cdots & y_{10}^2 \\
				\vdots & \vdots & \ddots & \vdots    \\
				y_1^k  & y_2^k  & \cdots & y_{10}^k
			\end{bmatrix}
		\end{equation}

	\item 正确答案 $T \in \R^{k\times 10}$

		\begin{equation}
			T = \begin{bmatrix}
				t^1    \\
				t^2    \\
				\vdots \\
				t^k
			\end{bmatrix} = \begin{bmatrix}
				t_1^1  & t_2^1  & \cdots & t_{10}^1 \\
				t_1^2  & t_2^2  & \cdots & t_{10}^2 \\
				\vdots & \vdots & \ddots & \vdots    \\
				t_1^k  & t_2^k  & \cdots & t_{10}^k
			\end{bmatrix}
		\end{equation}

\end{enumerate}

\subsection{神经网络参数}

\begin{enumerate}
	\item 输入层到隐藏层的权重 $W_1 \in \R^{784 \times 50}$

	\begin{equation}
		W_1 = \begin{bmatrix}
			w_1^1  & w_2^1  & \cdots & w_{50}^1 \\
			w_1^2  & w_2^2  & \cdots & w_{50}^2 \\
			\vdots & \vdots & \ddots & \vdots    \\
			w_1^{784} & w_2^{784} & \cdots & w_{50}^{784}
		\end{bmatrix}
	\end{equation}

	\item 输入层到隐藏层的偏置 $b_1 \in \R^{50}$

	\begin{equation}
		b_1 = \begin{bmatrix}
			b_1^1  & b_2^1  & \cdots & b_{50}^1
		\end{bmatrix}
	\end{equation}

	\item 隐藏层到输出层的权重 $W_2 \in \R^{50 \times 10}$

	\begin{equation}
		W_2 = \begin{bmatrix}
			w_1^1  & w_2^1  & \cdots & w_{10}^1 \\
			w_1^2  & w_2^2  & \cdots & w_{10}^2 \\
			\vdots & \vdots & \ddots & \vdots    \\
			w_1^{50} & w_2^{50} & \cdots & w_{10}^{50}
		\end{bmatrix}
	\end{equation}

	\item 隐藏层到输出层的偏置 $b_2 \in \R^{10}$

	\begin{equation}
		b_2 = \begin{bmatrix}
			b_2^1  & b_2^2  & \cdots & b_{10}^1
		\end{bmatrix}
	\end{equation}

\end{enumerate}

\subsection{神经网络参数的处理}

\begin{enumerate}
	\item 第 $t$ 步迭代的参数向量 $\theta^t \in \R^{39760}$ \footnote{其中 $39760 = 784 \times 50 + 50 + 50 \times 10 + 10$}

	\begin{equation}
		\theta^t = (\theta_1^t, \theta_2^t, \cdots, \theta_{39760}^t)
	\end{equation}

	\item 每一步迭代的参数向量的 Jacobi 矩阵 $Df(\theta^t) \in \R^{39760 \times 39760}$

	\begin{equation}
		Df(\theta^t) = \begin{bmatrix}
			\frac{\partial \theta_1^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_1^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_1^{t+1}}{\partial \theta_{39760}^t} \\
			\frac{\partial \theta_2^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_2^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_2^{t+1}}{\partial \theta_{39760}^t} \\
			\vdots                                              & \vdots                                              & \ddots & \vdots                                              \\
			\frac{\partial \theta_{39760}^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_{39760}^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_{39760}^{t+1}}{\partial \theta_{39760}^t}
		\end{bmatrix}
	\end{equation}
		
\end{enumerate}

\section{神经网络的结构}

\subsection{输入层}

输入层的维度为 $784$，即 $28 \times 28$ 的图片的 $784$ 个像素。

\subsection{隐藏层}

隐藏层的维度为 $50$。

先进行线性变换

\begin{equation} \label{eq:3}
	A_1 = X W_1 + b_1
\end{equation}

再经 $sigmoid$ 激活函数处理

\begin{equation} \label{eq:4}
	Z_1 = sigmoid(A_1) = sigmoid(\begin{bmatrix}
		a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
		a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
		\vdots & \vdots & \ddots & \vdots    \\
		a_1^k  & a_2^k  & \cdots & a_{50}^k
		\end{bmatrix}) = \begin{bmatrix}
			\frac{1}{1 + \exp(a_1^1)} & \frac{1}{1 + \exp(a_2^1)} & \cdots & \frac{1}{1 + \exp(a_{50}^1)} \\
			\frac{1}{1 + \exp(a_1^2)} & \frac{1}{1 + \exp(a_2^2)} & \cdots & \frac{1}{1 + \exp(a_{50}^2)} \\
			\vdots                      & \vdots                      & \ddots & \vdots                      \\
			\frac{1}{1 + \exp(a_1^k)} & \frac{1}{1 + \exp(a_2^k)} & \cdots & \frac{1}{1 + \exp(a_{50}^k)}
		\end{bmatrix}
\end{equation}

\subsection{输出层}

输出层的维度为 $10$

先进行线性变换

\begin{equation} \label{eq:5}
	A_2 = Z_1 W_2 + b_2
\end{equation}

再经过 $softmax$ 层

\begin{equation} \label{eq:6}
	Y = softmax(A_2) = softmax(\begin{bmatrix}
		{a'}^1 \\
		{a'}^2 \\
		\vdots  \\
		{a'}^k
		\end{bmatrix}) = \begin{bmatrix}
			\frac{\exp({{a'}^1_1})}{\sum_{l=1}^{10} \exp({{a'}^1_l})} & \frac{\exp({{a'}^1_2})}{\sum_{l=1}^{10} \exp({{a'}^1_l})} & \cdots & \frac{\exp({{a'}^1_{10}})}{\sum_{l=1}^{10} \exp({{a'}^1_l})} \\
			\frac{\exp({{a'}^2_1})}{\sum_{l=1}^{10} \exp({{a'}^2_l})} & \frac{\exp({{a'}^2_2})}{\sum_{l=1}^{10} \exp({{a'}^2_l})} & \cdots & \frac{\exp({{a'}^2_{10}})}{\sum_{l=1}^{10} \exp({{a'}^2_l})} \\
			\vdots                                                      & \vdots                                                      & \ddots & \vdots                                                      \\
			\frac{\exp({{a'}^k_1})}{\sum_{l=1}^{10} \exp({{a'}^k_l})} & \frac{\exp({{a'}^k_2})}{\sum_{l=1}^{10} \exp({{a'}^k_l})} & \cdots & \frac{\exp({{a'}^k_{10}})}{\sum_{l=1}^{10} \exp({{a'}^k_l})}
		\end{bmatrix}
\end{equation}

\subsection{损失函数}

我们用交叉熵作为损失函数，即

\begin{equation}
	L = - \frac{1}{k} \sum_{i=1}^{k} \sum_{j=1}^{10} y_j^i \log y_j^i
\end{equation}

\section{梯度和 Jacobi 矩阵}

第 $t$ 步迭代的参数为

\begin{equation}
	\theta^t = (W_1(t), b_1(t), W_2(t), b_2(t)) = (\theta_1^t, \theta_2^t, \cdots, \theta_{39760}^t)
\end{equation}

此时对应的梯度为

\begin{equation}
	grad(t) = (\frac{\partial L}{\partial \theta_1^t}, \frac{\partial L}{\partial \theta_2^t}, \cdots, \frac{\partial L}{\partial \theta_{39760}^t}) = (\frac{\partial L}{\partial W_1^t}, \frac{\partial L}{\partial b_1^t}, \frac{\partial L}{\partial W_2^t}, \frac{\partial L}{\partial b_2^t})
\end{equation}


神经网络迭代过程即

\begin{equation}
	\theta^{t+1} = \theta^t - \alpha \cdot grad(t)
\end{equation}

也即

\begin{equation}
	\begin{aligned}
		W_1^{t+1} & = W_1^t - \alpha \cdot \frac{\partial L}{\partial W_1^t} \\
		b_1^{t+1} & = b_1^t - \alpha \cdot \frac{\partial L}{\partial b_1^t} \\
		W_2^{t+1} & = W_2^t - \alpha \cdot \frac{\partial L}{\partial W_2^t} \\
		b_2^{t+1} & = b_2^t - \alpha \cdot \frac{\partial L}{\partial b_2^t}
	\end{aligned}
\end{equation}

因此迭代的 Jacobi 矩阵为

\begin{equation}
	J(t) = \begin{bmatrix}
		\frac{\partial \theta_1^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_1^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_1^{t+1}}{\partial \theta_{39760}^t} \\
		\frac{\partial \theta_2^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_2^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_2^{t+1}}{\partial \theta_{39760}^t} \\
		\vdots                                              & \vdots                                              & \ddots & \vdots                                              \\
		\frac{\partial \theta_{39760}^{t+1}}{\partial \theta_1^t} & \frac{\partial \theta_{39760}^{t+1}}{\partial \theta_2^t} & \cdots & \frac{\partial \theta_{39760}^{t+1}}{\partial \theta_{39760}^t}
	\end{bmatrix}
\end{equation}

也即

\begin{equation}
	J(t) = \begin{bmatrix}
		\frac{\partial W_1^{t+1}}{\partial W_1^t} & \frac{\partial W_1^{t+1}}{\partial b_1^t} & \frac{\partial W_1^{t+1}}{\partial W_2^t} & \frac{\partial W_1^{t+1}}{\partial b_2^t} \\
		\frac{\partial b_1^{t+1}}{\partial W_1^t} & \frac{\partial b_1^{t+1}}{\partial b_1^t} & \frac{\partial b_1^{t+1}}{\partial W_2^t} & \frac{\partial b_1^{t+1}}{\partial b_2^t} \\
		\frac{\partial W_2^{t+1}}{\partial W_1^t} & \frac{\partial W_2^{t+1}}{\partial b_1^t} & \frac{\partial W_2^{t+1}}{\partial W_2^t} & \frac{\partial W_2^{t+1}}{\partial b_2^t} \\
		\frac{\partial b_2^{t+1}}{\partial W_1^t} & \frac{\partial b_2^{t+1}}{\partial b_1^t} & \frac{\partial b_2^{t+1}}{\partial W_2^t} & \frac{\partial b_2^{t+1}}{\partial b_2^t}
	\end{bmatrix}
\end{equation}

\section{Jacobi 矩阵的计算}

\subsection{基本性质}

\subsubsection{sigmoid 函数的 Jacobi 矩阵}

假设 $A \in \R^{n\times m}$ 是同一个矩阵，则 $\frac{\partial (sigmoid(A))}{\partial A}$ 的计算公式为

\begin{equation}
	\frac{\partial (sigmoid(A))}{\partial A} = diag\{\frac{d (sigmoid(x_1^1))}{d x_1^1}, \frac{d (sigmoid(x_2^1))}{d x_2^1}, \cdots, \frac{d (sigmoid(x_m^n))}{d x_m^n} \}
\end{equation}
	
\subsubsection{softmax 函数的 Jacobi 矩阵}

假设 A 是一个矩阵，则 $\frac{\partial (softmax(A))}{\partial A}$ 的计算公式为

\begin{equation}
	\frac{\partial (softmax(A))}{\partial A} = softmax(A) \cdot (I - softmax(A))
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial W_1(t+1)}{\partial W_1(t)} & = \frac{\partial (W_1(t) - \alpha \cdot \frac{\partial L}{\partial W_1(t)})}{\partial W_1(t)} \\
		& = I - \alpha \frac{\partial^2 L}{\partial W_1 \partial W_1} \\
		& = I - \alpha \frac{\partial}{\partial W_1} \left( \frac{\partial L}{\partial W_1} \right) \\
		& = I - \alpha \frac{\partial}{\partial W_1} \left( \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial A_2} \frac{\partial A_2}{\partial W_1} \right) \\
		& = I - \alpha \frac{\partial}{\partial W_1} \left( \frac{\partial L}{\partial Y} \frac{\partial Y}{\partial A_2} \right) W_2^T Z_1^T \\
		& = I - \alpha \left( \frac{\partial}{\partial W_1} \left( \frac{\partial L}{\partial Y} \right) \frac{\partial Y}{\partial A_2} + \frac{\partial L}{\partial Y} \frac{\partial}{\partial W_1} \left( \frac{\partial Y}{\partial A_2} \right) \right) W_2^T Z_1^T
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {W_1}'}{\partial b_1} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right) X
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {W_1}'}{\partial W_2} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			x_1^1  & x_2^1  & \cdots & x_{784}^1 \\
			x_1^2  & x_2^2  & \cdots & x_{784}^2 \\
			\vdots & \vdots & \ddots & \vdots    \\
			x_1^k  & x_2^k  & \cdots & x_{784}^k
		\end{bmatrix} \right) \\
		\frac{\partial {W_1}'}{\partial b_2} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {b_1}'}{\partial W_1} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			x_1^1  & x_2^1  & \cdots & x_{784}^1 \\
			x_1^2  & x_2^2  & \cdots & x_{784}^2 \\
			\vdots & \vdots & \ddots & \vdots    \\
			x_1^k  & x_2^k  & \cdots & x_{784}^k
		\end{bmatrix} \right) \\
		\frac{\partial {b_1}'}{\partial b_1} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {b_1}'}{\partial W_2} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			x_1^1  & x_2^1  & \cdots & x_{784}^1 \\
			x_1^2  & x_2^2  & \cdots & x_{784}^2 \\
			\vdots & \vdots & \ddots & \vdots    \\
			x_1^k  & x_2^k  & \cdots & x_{784}^k
		\end{bmatrix} \right) \\
		\frac{\partial {b_1}'}{\partial b_2} & = - \alpha \left( softmax'(A_2) W_2^T sigmoid'(A_1) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {W_2}'}{\partial W_1} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
			a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
			\vdots & \vdots & \ddots & \vdots   \\
			a_1^k  & a_2^k  & \cdots & a_{50}^k
		\end{bmatrix} \right) \\
		\frac{\partial {W_2}'}{\partial b_1} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {W_2}'}{\partial W_2} & = I - \alpha \frac{\partial^2 Y}{\partial W_2 \partial W_2}                                                   \\
		                                     & = I - \alpha \frac{\partial}{\partial W_2} \left( softmax'(A_2) A_1 \right)                                   \\
		                                     & = I - \alpha \frac{\partial}{\partial W_2} \left( softmax'(A_2) \right) A_1                                   \\
		                                     & = I - \alpha \left( \frac{\partial softmax'(A_2)}{\partial A_2} \frac{\partial A_2}{\partial W_2} \right) A_1 \\
		                                     & = I - \alpha \left( softmax'(A_2) \frac{\partial A_2}{\partial W_2} \right) A_1                               \\
		                                     & = I - \alpha \left( softmax'(A_2) \begin{bmatrix}
			a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
			a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
			\vdots & \vdots & \ddots & \vdots   \\
			a_1^k  & a_2^k  & \cdots & a_{50}^k
		\end{bmatrix} \right) A_1
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {W_2}'}{\partial b_2} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {b_2}'}{\partial W_1} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
			a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
			\vdots & \vdots & \ddots & \vdots   \\
			a_1^k  & a_2^k  & \cdots & a_{50}^k
		\end{bmatrix} \right) \\
		\frac{\partial {b_2}'}{\partial b_1} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

\begin{equation}
	\begin{aligned}
		\frac{\partial {b_2}'}{\partial W_2} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			a_1^1  & a_2^1  & \cdots & a_{50}^1 \\
			a_1^2  & a_2^2  & \cdots & a_{50}^2 \\
			\vdots & \vdots & \ddots & \vdots   \\
			a_1^k  & a_2^k  & \cdots & a_{50}^k
		\end{bmatrix} \right) \\
		\frac{\partial {b_2}'}{\partial b_2} & = - \alpha \left( softmax'(A_2) \begin{bmatrix}
			1      & 1      & \cdots & 1      \\
			1      & 1      & \cdots & 1      \\
			\vdots & \vdots & \ddots & \vdots \\
			1      & 1      & \cdots & 1
		\end{bmatrix} \right)
	\end{aligned}
\end{equation}

将以上 $4\times 4 = 16$ 个 Jacobi 矩阵计算公式代入 $Df(W_1, b_1, W_2, b_2)$ 的定义，即得到 $Df(W_1, b_1, W_2, b_2)$ 的表达式

\end{document}