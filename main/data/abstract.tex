% !TeX root = ../thuthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}

  在不稳定神经网络中，梯度爆炸和消失问题限制了反向传播算法的有效性。随着网络层数增加，梯度可能会呈指数级变化，导致训练不稳定和性能下降。本文回顾了不稳定神经网络的理论基础，包括李雅普诺夫谱和向量的概念，强调其对研究神经网络动态特性的作用。通过编程计算两个神经网络的李雅普诺夫谱，展示了 QR 方法的具体实现，并验证了“中间向量”的对偶性问题，指出固定内积可能是探究神经网络动态特性的重要参数。
  
  % 关键词用“英文逗号”分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {神经网络, 反向传播, 李雅普诺夫谱},
  }
\end{abstract}

\begin{abstract*}
  
  In unstable neural networks, the issues of exploding and vanishing gradients limit the effectiveness of the backpropagation algorithm. As the number of network layers increases, gradients may change exponentially, leading to instability during training and a decline in model performance. This paper reviews the theoretical foundations of unstable neural networks, including the concepts of Lyapunov exponents and vectors, emphasizing their role in studying the dynamic characteristics of neural networks. By computing the Lyapunov spectra for two neural networks using programming, the QR method's implementation is demonstrated, and the duality problem of "intermediate vectors" is verified. The fixed inner product is highlighted as a potentially important parameter for exploring the dynamic properties of neural networks.
  
  % Use comma as separator when inputting
  \thusetup{
    keywords* = {Neural Network, Backpropagation, Lyapunov Spectrum},
  }
\end{abstract*}
