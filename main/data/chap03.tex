% !TeX root = ../thuthesis-example.tex

\chapter{不稳定神经网络}

本章将重点讨论不稳定神经网络，并结合文献中的相关研究，探讨若干已经提出的解决方法，包括权重初始化方法、伴随噪声法和核微分方法。

基于动力系统中的双曲假设和相关理论，Ni 等提出了等变散度公式和快速响应算法，给出了参数梯度（即线性响应）的逐点良定义的公式，从而实现了使用轨道对参数梯度进行采样\cite{ni2023recursive}\cite{ni2023fast}\cite{ni2020fast}。但是如果要使用在机器学习问题中，仍需突破双曲性的限制。而双曲性的重要表征就是李雅普诺夫谱。

\section{梯度爆炸}

在神经网络的训练过程中，梯度爆炸问题是影响网络训练效率和效果的主要障碍之一。梯度爆炸较多发生于深层神经网络，特别是进行反向传播时，梯度值可能会因为连续的链式法则计算而指数增长，导致数值不稳定和训练失败。

梯度爆炸问题可以通过一个简单的深层神经网络训练过程来说明：设一个多层感知器（MLP），其损失函数为 \( L \)，网络的权重为 \(\mathbf{W}\)，每层的输出为 \(\mathbf{a}_l\)：
\begin{equation}
  \mathbf{a}_{l+1} = \sigma(\mathbf{W}_l \mathbf{a}_l + \mathbf{b}_l).
\end{equation}

其中，\(\sigma\) 是激活函数，\(\mathbf{b}_l\) 是第 \(l\) 层的偏置。

在反向传播过程中，需要计算损失函数 \(L\) 对权重 \(\mathbf{W}_l\) 的梯度：
\begin{equation}
  \frac{\partial L}{\partial \mathbf{W}_l} = \delta_{l+1} \mathbf{a}_l^T.
 \end{equation}

其中，\(\delta_{l+1}\) 是误差项，定义为：
\begin{equation}
  \delta_{l+1} = \frac{\partial L}{\partial \mathbf{a}_{l+1}} \odot \sigma'(\mathbf{z}_{l+1}).
 \end{equation}

通过链式法则，误差项 \(\delta_l\) 的更新为：
\begin{equation}
  \delta_l = (\mathbf{W}_l^T \delta_{l+1}) \odot \sigma'(\mathbf{z}_l).
\end{equation}

如果有正的李雅普诺夫指数，则说明随着该神经网络的层数加深，上述过程会导致梯度的累积乘积，其中每一项都将会放大误差，最终使得梯度在反向传播过程中指数增长，导致梯度爆炸。

\section{权重初始化}

为了解决梯度爆炸的问题，人们提出了一系列方法，其中权重初始化方法最为常用。这些手段旨在通过合适的初始化策略，使得梯度在反向传播过程中保持稳定，避免梯度爆炸的发生。

下面介绍 Xavier 初始化和 He 初始化两种常见的权重初始化方法，并简述其原理和应用。

\subsection{Xavier 初始化}

Xavier 初始化（也称为 Glorot 初始化）是由 Xavier Glorot 和 Yoshua Bengio 在 2010 年提出的一种权重初始化方法。该方法旨在使网络层的输入和输出的方差保持一致，从而在前向传播和反向传播过程中，信号能够有效传递。

在 Xavier 初始化中，权重 \( W_l \) 的初始化遵循以下分布：
\begin{equation}
  W_l \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{l-1} + n_l}}, \sqrt{\frac{6}{n_{l-1} + n_l}}\right).
\end{equation}

或
\begin{equation}
  W_l \sim \mathcal{N}\left(0, \frac{2}{n_{l-1} + n_l}\right).
\end{equation}

其中，\( n_{l-1} \) 是第 \( l-1 \) 层的神经元数量，\( n_l \) 是第 \( l \) 层的神经元数量。前者使用均匀分布，后者使用正态分布。

Xavier 初始化的基本思想是通过选择合适的初始权重范围，使得每层输出的方差接近输入的方差，从而在训练的初始阶段避免信号的过度放大或缩小。

\subsection{He 初始化}

He 初始化（也称为 Kaiming 初始化）是由 Kaiming He 等人在 2015 年提出的一种改进的权重初始化方法，主要针对使用 ReLU 激活函数的神经网络。在 ReLU 激活函数下，输出的方差会受到输入方差的影响，因此需要更大的初始权重范围。

在 He 初始化中，权重 \( W_l \) 的初始化遵循以下分布：
\begin{equation}
  W_l \sim \mathcal{N}\left(0, \frac{2}{n_{l-1}}\right).
\end{equation}

或
\begin{equation}
  W_l \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{l-1}}}, \sqrt{\frac{6}{n_{l-1}}}\right).
\end{equation}

其中，\( n_{l-1} \) 是第 \( l-1 \) 层的神经元数量。与 Xavier 初始化相比，He 初始化在方差上增加了一倍，从而更加适应 ReLU 激活函数的特性。

\section{伴随噪声法}

除去权重初始化方法，伴随噪声法是另一种用于解决梯度爆炸问题的有效方法。

伴随噪声法出现的时间更晚，它通过引入伴随变量和李雅普诺夫分析来稳定反向传播过程，从而降低梯度爆炸的发生概率。该方法在复杂动态系统的控制中已有广泛应用，最近被引入到神经网络的训练中，以应对深层网络中的梯度爆炸问题。

\subsection{理论背景}

神经网络的反向传播过程中，梯度的计算依赖于复合函数求导的链式法则，具体现为层与层之间的梯度乘积。这种乘积会导致梯度的指数级别增长或减小，从而引发梯度爆炸或梯度消失问题。

为了解决这一问题，尝试引入伴随变量和李雅普诺夫分析，通过调整梯度计算过程间接控制梯度增长。

\subsection{伴随变量的引入}

设伴随变量 \( \mathbf{u}_l \) 是通过以下李雅普诺夫方程定义的：
\begin{equation}
  \mathbf{u}_l = \mathbf{Q}_l + \mathbf{A}_l \mathbf{u}_{l+1} \mathbf{A}_l^T.
\end{equation}

其中，\( \mathbf{Q}_l \) 是对称正定矩阵，\( \mathbf{A}_l \) 是系统矩阵。伴随变量 \( \mathbf{u}_l \) 捕捉了系统在反向传播过程中积累的数值不稳定性。

\subsection{梯度计算的调整}

在每一步反向传播中，利用伴随变量来调整梯度计算。具体而言，传统的梯度计算公式为：
\begin{equation}
  \frac{\partial L}{\partial \mathbf{W}_l} = \delta_{l+1} \mathbf{a}_l^T.
\end{equation}

其中，\( \delta_{l+1} \) 是第 \( l+1 \) 层的误差项，\( \mathbf{a}_l \) 是第 \( l \) 层的激活输出。在伴随噪声法中，通过伴随变量 \( \mathbf{u}_l \) 来修正梯度计算公式：
\begin{equation}
  \frac{\partial L}{\partial \mathbf{W}_l} = \mathbf{u}_l (\delta_{l+1} \mathbf{a}_l^T).
\end{equation}

该修正公式通过伴随变量调整梯度的增长，使得梯度在反向传播过程中得到有效控制。

\subsection{李雅普诺夫方程的求解}

李雅普诺夫方程在动态系统中用于分析系统的稳定性，其形式为：
\begin{equation}
  \mathbf{A} \mathbf{P} + \mathbf{P} \mathbf{A}^T + \mathbf{Q} = 0.
\end{equation}

对于李雅普诺夫方程，本节仅仅是简单介绍，与第四章中的李雅普诺夫谱的计算并无直接关联。

\section{核微分方法}

\subsection{方法简介}

另一种解决梯度爆炸问题的方法为赫微分方法。

核微分方法通过将梯度计算问题转换为核函数的操作，从而平滑梯度并减少梯度爆炸的风险。

设核函数 \(k(\mathbf{x}, \mathbf{y})\) 满足 Mercer 定理，即满足正定性和对称性。我们通过构造核矩阵 \(\mathbf{K}\) 来替代直接的梯度计算：
\begin{equation}
  \mathbf{K}_{ij} = k(\mathbf{x}_i, \mathbf{x}_j).
\end{equation}

在反向传播过程中，利用核矩阵来平滑梯度：
\begin{equation}
  \frac{\partial L}{\partial \mathbf{W}_l} = \mathbf{K} \mathbf{g}.
\end{equation}

其中，\(\mathbf{g}\) 是传统方法计算得到的梯度。

核微分方法的关键在于选择合适的核函数 \(k(\mathbf{x}, \mathbf{y})\)，例如高斯核或多项式核。通过核函数的平滑作用，能够减小梯度的波动，从而减轻或消除梯度爆炸问题。

\subsection{核函数选取}

\begin{enumerate}

\item 高斯核：
\begin{equation}
  k(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{||\mathbf{x} - \mathbf{y}||^2}{2\sigma^2}\right).
\end{equation}

其中，\(\sigma\) 是核的宽度参数。

\item 多项式核：
\begin{equation}
  k(\mathbf{x}, \mathbf{y}) = (\mathbf{x}^T \mathbf{y} + c)^d.
\end{equation}

其中，\(c\) 是常数，\(d\) 是多项式的度数。

\end{enumerate}

总结起来，核微分方法通过引入核函数，平滑了梯度变化，使得梯度在反向传播过程中不易发生爆炸。同时，这种方法也能够保持梯度信息，从而提高训练效率和效果。核微分方法是一种较为优秀的梯度控制方法。