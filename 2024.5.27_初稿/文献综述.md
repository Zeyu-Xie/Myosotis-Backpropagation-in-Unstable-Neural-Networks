反向传播算法（Backpropagation）在训练神经网络中占据核心地位。然而，随着网络结构的复杂化，特别是在递归神经网络（RNN）和深层网络中，训练过程面临许多困难，包括梯度消失和梯度爆炸问题【Pascanu2012】。这些问题导致训练时间延长，难以找到优化解【Ioffe2015】。

为了应对这些挑战，许多研究提出了不同的方法。Ni（2022）介绍了一种通过对共变向量场的伴随影子操作（Adjoint Shadowing Operator）来推广反向传播方法的研究。这种方法适用于离散时间和连续时间的超混沌系统，能够有效计算长时间统计量对系统参数的导数，并具有与传统反向传播算法相似的计算效率【Ni2022】。此外，Ni（2023）提出了一个无需传播矢量或协变向量的无传播算法，避免了梯度爆炸、维度诅咒和非超混沌性的影响。这一算法在混沌神经网络中的应用显示了其在非超混沌系统中的线性响应近似能力【Ni2023】。

在计算Lyapunov指数方面，Geist等（1990）比较了不同的离散和连续方法，讨论了这些方法在效率和准确性方面的差异。他们强调了一些方法因计算时间长和数值不稳定而不推荐使用【Geist1990】。与之相对，Bremen等（1997）开发了一种基于QR分解的高效且数值稳定的方法，验证了该方法在收敛性、准确性和效率方面的优越性【Bremen1997】。

针对深度神经网络，Storm等（2024）研究了小输入扰动如何影响网络输出，利用有限时间Lyapunov指数（Finite-Time Lyapunov Exponents）揭示了深度网络在输入空间中的几何结构。这一研究展示了深度网络学习能力的基本机制，尤其是在输入空间中形成不同类区域的几何结构【Storm2024】。

此外，Ni（2019）利用可压缩流体模拟分析了三维圆柱流的超混沌性、影子方向和敏感性，证明了影子方法在一般混沌流体问题中的有效性【Ni2019】。他还开发了非侵入式最小二乘伴随影子（NILSAS）算法，通过计算伴随影子方向进行混沌系统的敏感性分析，展示了在Lorenz 63系统和三维圆柱流上的应用【Ni2018】。

总结来看，尽管在不稳定神经网络中的反向传播算法面临诸多挑战，但通过引入新的算法和改进现有方法，研究人员在提高训练效率和解决梯度问题方面取得了显著进展。这些研究不仅深化了我们对神经网络训练机制的理解，还为实际应用中的网络优化提供了新的思路。